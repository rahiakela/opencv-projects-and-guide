{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-image-channels-splitting-and-merging.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpG0vGiq5aXwVu4HbnwypM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/opencv-projects-and-guide/blob/main/mastering-opencv4-with-python/5-image-processing-techniques/1_image_channels_splitting_and_merging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_eVD0XW2UUe"
      },
      "source": [
        "## Image channels splitting and merging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uapZVlhy2VKu"
      },
      "source": [
        "With the development of computer vision, machine learning, and deep learning, face recognition has become a hot topic. **Face recognition can be applied to a wide range of uses, including crime prevention, surveillance, forensic applications, biometrics, and, more recently, in social networks.** Automatic face recognition has various challenges, such as occlusions, appearance variations, expression, aging, and scale variations. Following its success with object recognition, CNNs have been widely used for face recognition.\n",
        "\n",
        "In this notebook, we will see the functionality that OpenCV offers in connection with face recognition, and will also explore some deep learning approaches, which can be easily integrated into your computer vision projects to perform state-of-the-art face recognition results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nlADkw85Lr-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbO7mA1J5Ncb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1282aa-c57c-473e-c4c9-7845a80707e6"
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install dlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dlib in /usr/local/lib/python3.7/dist-packages (19.18.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwcEei0-7ub7"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import dlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOmqa-kY5P6f"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/dlib_face_recognition_resnet_model_v1.dat\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/shape_predictor_5_face_landmarks.dat\n",
        "\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/jared_1.jpg\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/jared_2.jpg\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/jared_3.jpg\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/jared_4.jpg\n",
        "wget https://github.com/PacktPublishing/Mastering-OpenCV-4-with-Python/raw/master/Chapter11/01-chapter-content/face_recognition/obama.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VasdYuQTSaKl"
      },
      "source": [
        "def display_images(images, columns=5, width=20, height=8, max_images=15, label_wrap_length=50, label_font_size=8):\n",
        "\n",
        "    if not images:\n",
        "        print(\"No images to display.\")\n",
        "        return \n",
        "\n",
        "    if len(images) > max_images:\n",
        "        print(f\"Showing {max_images} images of {len(images)}:\")\n",
        "        images=images[0:max_images]\n",
        "\n",
        "    height = max(height, int(len(images) / columns) * height)\n",
        "    plt.figure(figsize=(width, height))\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "        plt.imshow(image)\n",
        "\n",
        "        if hasattr(image, 'filename'):\n",
        "            title=image.filename\n",
        "            if title.endswith(\"/\"): title = title[0:-1]\n",
        "            title=os.path.basename(title)\n",
        "            title=textwrap.wrap(title, label_wrap_length)\n",
        "            title=\"\\n\".join(title)\n",
        "            plt.title(title, fontsize=label_font_size); \n",
        "\n",
        "def display_images2(images, names, columns=5, width=20, height=8, max_images=15, label_wrap_length=50, label_font_size=8):\n",
        "\n",
        "    if not images:\n",
        "        print(\"No images to display.\")\n",
        "        return \n",
        "\n",
        "    if len(images) > max_images:\n",
        "        print(f\"Showing {max_images} images of {len(images)}:\")\n",
        "        images=images[0:max_images]\n",
        "\n",
        "    height = max(height, int(len(images) / columns) * height)\n",
        "    plt.figure(figsize=(width, height))\n",
        "    for i, image in enumerate(images):\n",
        "\n",
        "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(names[i], fontsize=label_font_size); \n",
        "\n",
        "        if hasattr(image, 'filename'):\n",
        "            title=image.filename\n",
        "            if title.endswith(\"/\"): title = title[0:-1]\n",
        "            title=os.path.basename(title)\n",
        "            title=textwrap.wrap(title, label_wrap_length)\n",
        "            title=\"\\n\".join(title)\n",
        "            plt.title(title, fontsize=label_font_size); "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYEd5lr67ypU"
      },
      "source": [
        "## Face recognition with dlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU18LNxV71ku"
      },
      "source": [
        "**Dlib offers a high-quality face recognition algorithm based on deep learning. Dlib implements a face recognition algorithm that offers state-of-the-art accuracy.** More specifically, the model has an accuracy of 99.38% on the labeled faces in the wild database.\n",
        "\n",
        "The implementation of this algorithm is based on the ResNet-34 network proposed in the paper Deep Residual Learning for Image Recognition (2016), which was trained using three million faces.\n",
        "\n",
        "**This network is trained in a way that generates a 128-dimensional (128D) descriptor, used to quantify the face. The training step is performed using triplets.**\n",
        "\n",
        "**A single triplet training example is composed of three images. Two of them correspond to the same person. The network generates the 128D descriptor for each of the images, slightly modifying the neural network weights in order to make the two vectors that correspond to the same person closer and the feature vector from the other person further away.** The triplet loss function\n",
        "formalizes this and tries to push the 128D descriptor of two images of the same person closer together, while pulling the 128D descriptor of two images of different people further apart.\n",
        "\n",
        "This process is repeated millions of times for millions of images of thousands of different people and finally, it is able to generate a 128D descriptor for each person. So, the final 128D descriptor is good encoding for the following reasons:\n",
        "\n",
        "- The generated 128D descriptors of two images of the same person are quite\n",
        "similar to each other.\n",
        "\n",
        "- The generated 128D descriptors of two images of different people are very\n",
        "different.\n",
        "\n",
        "**Therefore, making use of the dlib functionality, we can use a pre-trained model to map a face into a 128D descriptor. Afterward, we can use these feature vectors to perform face recognition.**\n",
        "\n",
        "Let's see how to calculate the 128D descriptor, used to\n",
        "quantify the face. The process is quite simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB2hOX3wIiwS"
      },
      "source": [
        "Now let's calculate the encodings for every face of the image.As you can guess, it calculate the 128D descriptor for each face in the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKwmArGCIjHp"
      },
      "source": [
        "# Load shape predictor, face enconder and face detector using dlib library\n",
        "pose_predictor_5_point = dlib.shape_predictor(\"shape_predictor_5_face_landmarks.dat\")\n",
        "face_encoder = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1.dat\")\n",
        "detector = dlib.get_frontal_face_detector()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XlO3h6aI7Xw"
      },
      "source": [
        "def face_encodings(face_image, number_of_times_to_upsample=1, num_jitters=1):\n",
        "  \"\"\"Returns the 128D descriptor for each face in the image\"\"\"\n",
        "\n",
        "  # Detect faces\n",
        "  face_locations = detector(face_image, number_of_times_to_upsample)\n",
        "  # Detected landmarks\n",
        "  raw_landmarks = [pose_predictor_5_point(face_image, face_location) for face_location in face_locations]\n",
        "\n",
        "  # Calculate the face encoding for every detected face using the detected landmarks for each one\n",
        "  return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XwXPRU_CiMp"
      },
      "source": [
        "As you can see, the key point is to calculate the face encoding for every detected face using the detected landmarks for each one, calling dlib\n",
        "the `face_encoder.compute_face_descriptor()` function.\n",
        "\n",
        "The `num_jitters parameter` sets the number of times each face will be randomly jittered, and the average 128D descriptor calculated each time will be returned.\n",
        "\n",
        "Once the faces are encoded, the next step is to perform the recognition.\n",
        "\n",
        "The recognition can be easily computed using some kind of distance metrics computed using the `128D` descriptors. Indeed, if two face descriptor vectors have a Euclidean distance between them that is less than `0.6`, they can be considered to belong to the same person. Otherwise, they are from different people.\n",
        "\n",
        "The Euclidean distance can be calculated using numpy.linalg.norm().\n",
        "\n",
        "We compare four images against another image. To compare the faces, we have coded two functions: \n",
        "\n",
        "- `compare_faces()` and\n",
        "- `compare_faces_ordered()`. \n",
        "\n",
        "The compare_faces() function returns the distance when\n",
        "comparing a list of face encodings against a candidate to check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hgvrU7qFbcu"
      },
      "source": [
        "def compare_faces(encodings, encoding_to_check):\n",
        "  \"\"\"Returns the distances when comparing a list of face encodings against a candidate to check\"\"\"\n",
        "\n",
        "  return list(np.linalg.norm(encodings - encoding_to_check, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQj2_AKcFwAC"
      },
      "source": [
        "The compare_faces_ordered() function returns the ordered distances and the\n",
        "corresponding names when comparing a list of face encodings against a candidate to check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1tiEBXDFwsl"
      },
      "source": [
        "def compare_faces_ordered(encodings, face_names, encoding_to_check):\n",
        "  \"\"\"Returns the ordered distances and names when comparing a list of face encodings against a candidate to check\"\"\"\n",
        "\n",
        "  distances = list(np.linalg.norm(encodings - encoding_to_check, axis=1))\n",
        "\n",
        "  return zip(*sorted(zip(distances, face_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGKP2P5JF7cd"
      },
      "source": [
        "Therefore, the first step in comparing four images against another image is to load all of them and convert to RGB (dlib format):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOnrRR7qF9ga"
      },
      "source": [
        "# Load image\n",
        "known_image_1 = cv2.imread(\"jared_1.jpg\")\n",
        "known_image_2 = cv2.imread(\"jared_2.jpg\")\n",
        "known_image_3 = cv2.imread(\"jared_3.jpg\")\n",
        "known_image_4 = cv2.imread(\"obama.jpg\")\n",
        "unknown_image = cv2.imread(\"jared_4.jpg\")\n",
        "\n",
        "# Convert image from BGR (OpenCV format) to RGB (dlib format)\n",
        "known_image_1 = known_image_1[:, :, ::-1]\n",
        "known_image_2 = known_image_2[:, :, ::-1]\n",
        "known_image_3 = known_image_3[:, :, ::-1]\n",
        "known_image_4 = known_image_4[:, :, ::-1]\n",
        "unknown_image = unknown_image[:, :, ::-1]\n",
        "\n",
        "# Create names for each loaded image\n",
        "names = [\"Jared 1\", \"Jared 2\", \"Jared 3\", \"Obama\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3QbKLV_GKkr"
      },
      "source": [
        "The next step is to compute the encodings for each image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk4HCet4GLMm"
      },
      "source": [
        "# Create the encodings\n",
        "known_image_1_encoding = face_encodings(known_image_1)[0]\n",
        "known_image_2_encoding = face_encodings(known_image_2)[0]\n",
        "known_image_3_encoding = face_encodings(known_image_3)[0]\n",
        "known_image_4_encoding = face_encodings(known_image_4)[0]\n",
        "\n",
        "known_encodings = [known_image_1_encoding, known_image_2_encoding, known_image_3_encoding, known_image_4_encoding]\n",
        "\n",
        "unknown_encoding = face_encodings(unknown_image)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYAqVDHrG-8r"
      },
      "source": [
        "And finally, you can compare the faces using the previous functions. For example, let's make use of the `compare_faces_ordered()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpHTyWrQHBNV",
        "outputId": "71452c77-ba7c-4013-c0fd-7a71026770b0"
      },
      "source": [
        "computed_distances_ordered, ordered_names = compare_faces_ordered(known_encodings, names, unknown_encoding)\n",
        "print(computed_distances_ordered)\n",
        "print(ordered_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.3913188139970749, 0.3998326700265651, 0.41041538984538495, 0.9053701470825026)\n",
            "('Jared 3', 'Jared 1', 'Jared 2', 'Obama')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ddGAaKHq5t"
      },
      "source": [
        "The first three values are less than 0.6. This means that the first three imagescan be considered from the same person.\n",
        "\n",
        "The fourth value obtained means that the fourth image is not the same\n",
        "person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlxCItFiIPip",
        "outputId": "d4d007b0-c718-46c6-b58a-e040dceabf07"
      },
      "source": [
        "# Compare faces\n",
        "computed_distances = compare_faces(known_encodings, unknown_encoding)\n",
        "print(computed_distances)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.3998326700265651, 0.41041538984538495, 0.3913188139970749, 0.9053701470825026]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttz3J3JEcd8p"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/face-recognition.png?raw=1' width='800'/>\n",
        "\n",
        "We can see that the first three images can be considered from the same person (the obtained values are less than 0.6), while the fourth image can be\n",
        "considered from another person (the obtained value is greater than 0.6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cIta7vSCdWW"
      },
      "source": [
        "## Face recognition with face_recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIs26kJMCe1O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxtzbHFNBhiN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}